# -*- coding: utf-8 -*-
"""Used car price prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E98yS__VSV247UN1e2EKqObkmGZQsrCD

we are building a machine learning model to predict the selling price of the car.
Selling price is the target variable or dependent variable and all other are the independent variable

# Part 1: Data Preprocessing

dataset link: https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho?select=car+data.csv

## Importing the libraries and the dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('/content/car data.csv')

# To check if the dataset has added sucessfully
dataset.head()

dataset.shape

"""There are total 301 entries in this daatset and total 9 columns"""

dataset.columns

# It provide the info about the dataset as in the datatype of the columns
# null or non null values
dataset.info()

# Categorical columns in this dataset
dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

# numerical columns
dataset.select_dtypes(include=['float64','int64']).columns

len(dataset.select_dtypes(include=['float64','int64']).columns)

# It provides the statistical summary of the dataset
dataset.describe()

"""## Dealing with missing values"""

dataset.isnull().values.any()

dataset.isnull().values.sum()

"""This shows this dataset has no null values

## Restructure the dataset
"""

dataset.head()

"""By observing the dataset we can conclude that the car name is not related with the selling price so, we can drop this column"""

dataset = dataset.drop(columns='Car_Name')

dataset.head()

# Adding a column
dataset['Current_Year'] = 2024

dataset.head()

# add a column
dataset['Years_Old'] = dataset['Current_Year'] - dataset['Year']

dataset.head()

"""Now we do not need current year or year in our dataset so we drop these columns"""

dataset = dataset.drop(columns=['Current_Year','Year'])

dataset.head()

"""## Encoding the categorical data"""

dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

# Check the unique value in these columns
dataset['Fuel_Type'].nunique()

dataset['Seller_Type'].nunique()

dataset['Transmission'].nunique()

dataset.shape

# Apply one-hot encoding
dataset = pd.get_dummies(data=dataset, drop_first=True)

# Convert only the Boolean columns
bool_columns = dataset.select_dtypes(include=['bool']).columns
dataset[bool_columns] = dataset[bool_columns].astype(int)

dataset.head()

dataset.shape

"""##  Correlation matrix

To check the correlation of independent variables with the dependent variable first we create dataset_2 then we drop the dependent variable from that so, that we have a dataset only consisting of the independent variable then correlate this dataset with the dependent variable that is Selling_Price to see the relation plot
"""

dataset_2 = dataset.drop(columns='Selling_Price')

dataset_2.corrwith(dataset['Selling_Price']).plot.bar(
    figsize=(16,9),title='Correlating with selling price', grid=True
)

corr = dataset.corr()

# heatmap
plt.figure(figsize=(16,9))
sns.heatmap(corr, annot=True)

"""The more light the color the more is correlation and the black boxes are negatively correlated

## Splitting the dataset

Before Spliting the dataset we have to identify x and y that is features of matrix and then we have to split the dataset into test and train set
"""

dataset.head()

# matrix of features
x = dataset.drop(columns='Selling_Price')

# target variable
y = dataset['Selling_Price']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=0)

"""80 percent is the tarining data and 20 percent is the test data"""

x_train.shape

y_train.shape

x_test.shape

y_test.shape

"""## Feature Scaling"""

# Not doing feature scaling in this

"""# Part 2: Building the model

## Multiple Linear regression Model
"""

# first import the class LinearRegression
# then create an instance of this class
# then train the model using .fit method

from sklearn.linear_model import LinearRegression
regressor_mlr = LinearRegression()
regressor_mlr.fit(x_train, y_train)

# predict a y value
y_pred = regressor_mlr.predict(x_test)

"""Now we have to evalute the model performes, using r_2 score,
the best possible score is 1 and it can be negative also
"""

from sklearn.metrics import r2_score

r2_score(y_test,y_pred)

"""## Random Forest Regression"""

from sklearn.ensemble import RandomForestRegressor
regressor_rf = RandomForestRegressor()
regressor_rf.fit(x_train,y_train)

y_pred = regressor_rf.predict(x_test)

# r2_score is also called coefficient of determination
# is a statistical measure that indicates how well the independent variables in a regression model
# explain the variability of the dependent variable

from sklearn.metrics import r2_score
r2_score(y_test,y_pred)

"""Random Forest r2_score is more close to 1 than to multiple linear regression therefore we finalize random forest regression as our model

# Part 3: Finding the optimal parameters using RandomizedSearchCv
"""

# Hyperparameter Tuning
from sklearn.model_selection import RandomizedSearchCV

parameters = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],  # Corrected here
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

random_cv = RandomizedSearchCV(estimator=regressor_rf, param_distributions=parameters, n_iter=10, scoring='neg_mean_absolute_error', cv=5, verbose=2, n_jobs=-1)
random_cv.fit(x_train, y_train)

random_cv.best_estimator_

random_cv.best_params_

"""# Part 4: Final model"""

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(max_depth=10, max_features='log2',
                                  n_estimators=500
    )
regressor.fit(x_train,y_train)

y_pred = regressor.predict(x_test)

from sklearn.metrics import r2_score
r2_score(y_test,y_pred)

"""# Part 5: Predicting a single observation"""

dataset.head()

# To get the predicted price we have to provide value to above each columns
observation = [[8,3500,0,5,1,0,0,1]]

regressor.predict(observation)

